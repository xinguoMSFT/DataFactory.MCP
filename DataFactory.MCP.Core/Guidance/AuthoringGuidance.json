{
    "version": "1.0",
    "description": "Authoring guidance for creating Fabric Dataflows",
    "dataTypes": [
        {
            "type": "type text",
            "mType": "Text.Type",
            "forNullable": "nullable Text.Type"
        },
        {
            "type": "type number",
            "mType": "Number.Type",
            "forNullable": "nullable Number.Type"
        },
        {
            "type": "Int64.Type",
            "mType": "Int64.Type",
            "forNullable": "nullable Int64.Type"
        },
        {
            "type": "Int32.Type",
            "mType": "Int32.Type",
            "forNullable": "nullable Int32.Type"
        },
        {
            "type": "type logical",
            "mType": "Logical.Type",
            "forNullable": "nullable Logical.Type"
        },
        {
            "type": "type date",
            "mType": "Date.Type",
            "forNullable": "nullable Date.Type"
        },
        {
            "type": "type datetime",
            "mType": "DateTime.Type",
            "forNullable": "nullable DateTime.Type"
        },
        {
            "type": "Currency.Type",
            "mType": "Currency.Type",
            "forNullable": "nullable Currency.Type"
        }
    ],
    "namingRules": {
        "simpleNames": "Use directly: MyQuery, Query1, SourceData",
        "specialCharacters": "Wrap in #\"\": #\"My Query\", #\"Data (2024)\"",
        "columnNamesWithSpaces": "Use {\"Column Name\"} in lists or [Column Name] for access"
    },
    "nextSteps": [
        "1. Author each query following the structure above",
        "2. Replace placeholders with actual values (workspaceId, lakehouseId, column names)",
        "3. Add connections for ALL sources AND destinations using 'add_connection_to_dataflow' tool",
        "4. Call 'validate_and_save_m_document' with the complete M document to save to dataflow"
    ],
    "gen2": {
        "pattern": "Gen2 FastCopy ETL Pipeline",
        "namingConvention": "Use '{SourceQueryName}_DataDestination' naming convention",
        "sectionAttribute": "[StagingDefinition = [Kind = \"FastCopy\"]]",
        "dataDestinationsAttribute": "[DataDestinations = {[Definition = [Kind = \"Reference\", QueryName = \"{destinationQueryName}\", IsNewTarget = true], Settings = [Kind = \"Automatic\", TypeSettings = [Kind = \"Table\"]]]}]",
        "steps": [
            {
                "step": 1,
                "action": "Add Section Header with FastCopy",
                "description": "Start with '[StagingDefinition = [Kind = \"FastCopy\"]]' followed by 'section Section1;'"
            },
            {
                "step": 2,
                "action": "Create Source Query with DataDestinations Attribute",
                "description": "Load data from source and add [DataDestinations] attribute linking to the destination query"
            },
            {
                "step": 3,
                "action": "Create DataDestination Query",
                "description": "Navigate to target table using '{tableName}_DataDestination' naming"
            }
        ],
        "tips": [
            "Gen2 uses [StagingDefinition = [Kind = \"FastCopy\"]] at the section level",
            "The source query must have [DataDestinations] attribute referencing the destination query",
            "DataDestination query should use '{SourceName}_DataDestination' naming convention",
            "Use Lakehouse.Contents([HierarchicalNavigation = null, CreateNavigationProperties = false, EnableFolding = false]) for destination",
            "Use optional navigation with '?' operator: Navigation{[Name = \"Table\"]}?[Data]?",
            "IsNewTarget = true creates the table if it doesn't exist",
            "Gen2 pattern is simpler: only 2 queries needed (source + destination)",
            "CRITICAL: You MUST add connections for BOTH source AND destination using 'add_connection_to_dataflow'. Find matching connections from 'list_connections' by type and path."
        ]
    },
    "gen1": {
        "pattern": "Gen1 Pipeline ETL",
        "namingConvention": "Use descriptive names",
        "stagingAttribute": "[Staging = \"DefaultModelStorage\"]",
        "defaultModelStorageQuery": "let DefaultModelStorage = Pipeline.DefaultModelStorage() in DefaultModelStorage",
        "steps": [
            {
                "step": 1,
                "action": "Create Source Query",
                "description": "Load data from source using the appropriate connector"
            },
            {
                "step": 2,
                "action": "Create Transformation Query (if needed)",
                "description": "Apply filters, column selections, type conversions, and computed columns"
            },
            {
                "step": 3,
                "action": "Create DefaultModelStorage Query",
                "description": "Required for write operations - use Pipeline.DefaultModelStorage()"
            },
            {
                "step": 4,
                "action": "Create Destination Query",
                "description": "Define the target table with schema"
            },
            {
                "step": 5,
                "action": "Create Column Mapping Query",
                "description": "Map source columns to destination columns"
            },
            {
                "step": 6,
                "action": "Create Write Query",
                "description": "Execute the data load using Pipeline.ExecuteAction with transaction"
            }
        ],
        "tips": [
            "Use #\"Name With Spaces\" syntax for query/column names with special characters",
            "Add 'nullable' prefix to destination column types: nullable Text.Type",
            "Use ? operator for optional navigation: Table{[Id = \"x\"]}?[Data]?",
            "The [Staging] attribute is required on write queries",
            "Gen1 requires more queries: DefaultModelStorage, explicit column mapping, Pipeline.ExecuteAction",
            "CRITICAL: You MUST add connections for BOTH source AND destination using 'add_connection_to_dataflow'. Find matching connections from 'list_connections' by type and path."
        ]
    },
    "sourceTypeKeywords": {
        "sql": [
            "sql server",
            "azure sql",
            "database"
        ],
        "csv": [
            "csv",
            "comma separated"
        ],
        "sharepoint": [
            "sharepoint"
        ],
        "odata": [
            "odata",
            "api"
        ],
        "web": [
            "web",
            "http",
            "url"
        ],
        "warehouse": [
            "warehouse"
        ],
        "lakehouse": [
            "lakehouse"
        ]
    },
    "transformationKeywords": [
        "transform",
        "convert",
        "change",
        "modify",
        "add column",
        "compute",
        "calculate"
    ],
    "filteringKeywords": [
        "filter",
        "where",
        "only",
        "exclude",
        "active",
        "valid",
        "remove"
    ],
    "aggregationKeywords": [
        "aggregate",
        "sum",
        "count",
        "average",
        "group",
        "total",
        "summarize"
    ]
}